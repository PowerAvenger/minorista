{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import zipfile\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descarga de un id (específicamente id600 con filtrado geo_ids=3)\n",
    "def download_esios_id(id, fecha_ini, fecha_fin, agrupacion):\n",
    "                       \n",
    "    cab = dict()\n",
    "    cab ['x-api-key'] = st.secrets['ESIOS_API_KEY']\n",
    "    url_id = 'https://api.esios.ree.es/indicators'\n",
    "    url = f'{url_id}/{id}?geo_ids[]=3&start_date={fecha_ini}T00:00:00&end_date={fecha_fin}T23:59:59&time_trunc={agrupacion}'\n",
    "    print(url)\n",
    "    datos_origen = requests.get(url, headers = cab).json()\n",
    "    \n",
    "    #arreglamos los datos origen    \n",
    "    datos =pd.DataFrame(datos_origen['indicator']['values'])\n",
    "    datos = (datos\n",
    "        .assign(datetime=lambda vh_: pd #formateamos campo fecha, desde un str con diferencia horaria a un naive\n",
    "            .to_datetime(vh_['datetime'],utc=True)  # con la fecha local\n",
    "            .dt\n",
    "            .tz_convert('Europe/Madrid')\n",
    "            .dt\n",
    "            .tz_localize(None)\n",
    "        )   \n",
    "        .loc[:,['datetime','value']]\n",
    "    )\n",
    "    datos['fecha'] = datos['datetime'].dt.date\n",
    "    datos['fecha'] = pd.to_datetime(datos['fecha'], format = '%d/%m/%Y')\n",
    "    datos['año'] = datos['datetime'].dt.year\n",
    "    datos['mes'] = datos['datetime'].dt.month\n",
    "    datos['dia'] = datos['datetime'].dt.day\n",
    "    datos['hora'] = datos['datetime'].dt.hour\n",
    "    meses_español = {1: \"ene\", 2: \"feb\", 3: \"mar\", 4: \"abr\", 5: \"may\", 6: \"jun\",\n",
    "                 7: \"jul\", 8: \"ago\", 9: \"sep\", 10: \"oct\", 11: \"nov\", 12: \"dic\"}\n",
    "    datos.rename(columns={'value':'spot'}, inplace=True)\n",
    "    datos['mes_nombre'] = datos['mes'].map(meses_español)\n",
    "    #datos = datos['fecha', 'año', 'mes', 'mes_nombre', 'dia', 'hora', 'spot' ]\n",
    "    datos.set_index('datetime', inplace = True)\n",
    "    \n",
    "    return datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 793: RRTT PBF RT3\n",
    "# 794: RRTT Tiempo Real RT6\n",
    "# 798: Banda Secundaria BS3\n",
    "# 799: Desvíos medidos EXD\n",
    "# 800: Saldo de desvíos DSV\n",
    "# 802: Saldo P.O.14.6 IN7\n",
    "# 1285: Control Factor de Potencia CFP\n",
    "# 1366: Incumplimiento energía de balance BALX\n",
    " \n",
    "# 803: No usado\n",
    "# 797: No usado\n",
    "# 1276 SI - No usado\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descarga de multi-ids (en este caso los ssaa)\n",
    "def download_esios_ids(indicadores, fecha_inicio, fecha_fin, time_trunc = 'hour'):\n",
    "      \n",
    "    # preparamos la cabecera a insertar en la llamada. Vease la necesidad de disponer el token de esios\n",
    "    cab = dict()\n",
    "    cab ['x-api-key'] = st.secrets['ESIOS_API_KEY']\n",
    "    # preparamos la url básica a la que se le añadiran los campos necesarios \n",
    "    end_point = 'https://api.esios.ree.es/indicators'\n",
    "    \n",
    "    # El procedimiento es sencillo: \n",
    "    # a) por cada uno de los indicadores configuraremos la url, según las indicaciones de la documentación.\n",
    "    # b) Hacemos la llamada y recogemos los datos en formato json.\n",
    "    # c) Añadimos la información a una lista\n",
    "    \n",
    "    lista=[]\n",
    "\n",
    "    for indicador in indicadores:\n",
    "        url = f'{end_point}/{indicador}?start_date={fecha_inicio}T00:00:00&end_date={fecha_fin}T23:59:59&time_trunc={time_trunc}'\n",
    "        print (url)\n",
    "        response = requests.get(url, headers=cab).json()\n",
    "        lista.append(pd.json_normalize(data=response['indicator'], record_path=['values'], meta=['id','name','short_name'], errors='ignore'))\n",
    "\n",
    "    # Devolvemos como salida de la función un df fruto de la concatenación de los elemenos de la lista\n",
    "    # Este procedimiento, con una sola concatenación al final, es mucho más eficiente que hacer múltiples \n",
    "    # concatenaciones.\n",
    "    \n",
    "    return pd.concat(lista, ignore_index=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.esios.ree.es/indicators/600?geo_ids[]=3&start_date=2023-01-01T00:00:00&end_date=2025-12-31T23:59:59&time_trunc=hour\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spot</th>\n",
       "      <th>fecha</th>\n",
       "      <th>año</th>\n",
       "      <th>mes</th>\n",
       "      <th>dia</th>\n",
       "      <th>hora</th>\n",
       "      <th>mes_nombre</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:00:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 01:00:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 02:00:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 03:00:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>ene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 04:00:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>ene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-06 19:00:00</th>\n",
       "      <td>126.02</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-06 20:00:00</th>\n",
       "      <td>117.01</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-06 21:00:00</th>\n",
       "      <td>105.51</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-06 22:00:00</th>\n",
       "      <td>91.96</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-06 23:00:00</th>\n",
       "      <td>76.60</td>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>mar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19104 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       spot      fecha   año  mes  dia  hora mes_nombre\n",
       "datetime                                                               \n",
       "2023-01-01 00:00:00    0.00 2023-01-01  2023    1    1     0        ene\n",
       "2023-01-01 01:00:00    0.00 2023-01-01  2023    1    1     1        ene\n",
       "2023-01-01 02:00:00    0.00 2023-01-01  2023    1    1     2        ene\n",
       "2023-01-01 03:00:00    0.00 2023-01-01  2023    1    1     3        ene\n",
       "2023-01-01 04:00:00    0.00 2023-01-01  2023    1    1     4        ene\n",
       "...                     ...        ...   ...  ...  ...   ...        ...\n",
       "2025-03-06 19:00:00  126.02 2025-03-06  2025    3    6    19        mar\n",
       "2025-03-06 20:00:00  117.01 2025-03-06  2025    3    6    20        mar\n",
       "2025-03-06 21:00:00  105.51 2025-03-06  2025    3    6    21        mar\n",
       "2025-03-06 22:00:00   91.96 2025-03-06  2025    3    6    22        mar\n",
       "2025-03-06 23:00:00   76.60 2025-03-06  2025    3    6    23        mar\n",
       "\n",
       "[19104 rows x 7 columns]"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generar_datos(fecha_ini, fecha_fin):\n",
    "    \n",
    "    # descargamos el spot\n",
    "    df_datos_spot = download_esios_id('600', fecha_ini, fecha_fin, 'hour')\n",
    "    # leemos el fichero de periodos horarios\n",
    "    df_periodos = pd.read_excel('periodos_horarios.xlsx', parse_dates = ['fecha'])\n",
    "    df_periodos['fecha'] = pd.to_datetime(df_periodos['fecha'], format = '%d/%m/%Y', errors = 'coerce')\n",
    "    # combinamos añadiendo dh3p y dh6p\n",
    "    df_datos = df_datos_spot.copy()\n",
    "    df_datos = df_datos.merge(df_periodos[['fecha', 'hora', 'dh_3p', 'dh_6p']], on = ['fecha', 'hora'], how = 'left')\n",
    "    orden_col = ['fecha', 'año', 'mes', 'mes_nombre', 'dia', 'hora', 'dh_3p', 'dh_6p', 'spot']\n",
    "    df_datos = df_datos[orden_col]\n",
    "    # descargamos los ssaa\n",
    "    ids_ssaa = [793, 794, 798, 799, 800, 802, 1285, 1366]\n",
    "    df_ssaa_origen = download_esios_ids(ids_ssaa, fecha_ini, fecha_fin, 'hour' )\n",
    "    df_ssaa = (df_ssaa_origen\n",
    "        .assign(datetime=lambda vh_: pd #formateamos campo fecha, desde un str con diferencia horaria a un naive\n",
    "            .to_datetime(vh_['datetime'],utc=True)  # con la fecha local\n",
    "            .dt\n",
    "            .tz_convert('Europe/Madrid')\n",
    "            .dt\n",
    "            .tz_localize(None)\n",
    "        )   \n",
    "        .loc[:,['datetime','value', 'id']]\n",
    "    )\n",
    "    df_ssaa['fecha'] = df_ssaa['datetime'].dt.date\n",
    "    df_ssaa['fecha'] = pd.to_datetime(df_ssaa['fecha'], format = '%d/%m/%Y')\n",
    "    df_ssaa['hora'] = df_ssaa['datetime'].dt.hour\n",
    "    # combinamos con el df de datos añadiendo los ssaa\n",
    "    df_ssaa = df_ssaa.copy()\n",
    "    df_ssaa = df_ssaa.groupby(['fecha', 'hora'], as_index=False)['value'].sum()\n",
    "    df_ssaa.rename(columns = {'value' : 'ssaa'}, inplace = True)\n",
    "    df_datos = df_datos.merge(df_ssaa[['fecha', 'hora', 'ssaa']], on = ['fecha', 'hora'], how = 'left')\n",
    "    df_datos = df_datos.copy()\n",
    "\n",
    "    # Cargar todas las tablas del archivo (incluyendo nombres de tablas definidas en Excel)\n",
    "    file_path = \"004 LUZ componentes regulados.xlsx\"\n",
    "    # Cargar todas las hojas\n",
    "    sheets_dict = pd.read_excel(file_path, sheet_name=None, engine=\"openpyxl\")\n",
    "\n",
    "    # Crear un diccionario con todas las tablas identificadas\n",
    "    tablas = {sheet_name: df for sheet_name, df in sheets_dict.items()}\n",
    "\n",
    "    # Acceder a cualquier tabla por su nombre de hoja\n",
    "    df_ppcc = tablas[\"PPCC\"]\n",
    "    df_osom = tablas['OSOM']\n",
    "    df_perdidas_boe = tablas['PERDIDAS']\n",
    "    df_pycs_energia = tablas['PYC_E']\n",
    "    df_datos['ppcc_2.0'] = None\n",
    "    for _, row in df_ppcc.iterrows():\n",
    "        mask = (df_datos[\"fecha\"] >= row[\"fecha_inicio\"]) & (df_datos[\"fecha\"] <= row[\"fecha_final\"]) & (df_datos[\"dh_3p\"] == row[\"periodo\"])\n",
    "        df_datos.loc[mask, \"ppcc_2.0\"] = row[\"2.0TD\"]\n",
    "    df_datos['ppcc_3.0'] = None\n",
    "    for _, row in df_ppcc.iterrows():\n",
    "        mask = (df_datos[\"fecha\"] >= row[\"fecha_inicio\"]) & (df_datos[\"fecha\"] <= row[\"fecha_final\"]) & (df_datos[\"dh_6p\"] == row[\"periodo\"])\n",
    "        df_datos.loc[mask, \"ppcc_3.0\"] = row[\"3.0TD\"]\n",
    "    df_datos['ppcc_6.1'] = None    \n",
    "    for _, row in df_ppcc.iterrows():\n",
    "        mask = (df_datos[\"fecha\"] >= row[\"fecha_inicio\"]) & (df_datos[\"fecha\"] <= row[\"fecha_final\"]) & (df_datos[\"dh_6p\"] == row[\"periodo\"])\n",
    "        df_datos.loc[mask, \"ppcc_6.1\"] = row[\"6.1TD\"]  \n",
    "    df_datos[['ppcc_2.0', 'ppcc_3.0', 'ppcc_6.1']] *= 1000\n",
    "    df_datos = df_datos.copy()\n",
    "    df_datos['osom'] = None\n",
    "    for _, row in df_osom.iterrows():\n",
    "        mask = (df_datos[\"fecha\"] >= row[\"fecha_inicio\"]) & (df_datos[\"fecha\"] <= row[\"fecha_final\"])\n",
    "        df_datos.loc[mask, \"osom\"] = row[\"osom\"]\n",
    "    df_datos['otros'] = 3.7\n",
    "    # Asignar la columna perd_2.0_boe a df_datos según el periodo dh_3p\n",
    "    df_datos[\"perd_2.0_boe\"] = df_datos[\"dh_3p\"].map(df_perdidas_boe.set_index(\"periodo\")[\"2.0TD\"])\n",
    "    df_datos[\"perd_3.0_boe\"] = df_datos[\"dh_6p\"].map(df_perdidas_boe.set_index(\"periodo\")[\"3.0TD\"])\n",
    "    df_datos[\"perd_6.1_boe\"] = df_datos[\"dh_6p\"].map(df_perdidas_boe.set_index(\"periodo\")[\"6.1TD\"])\n",
    "\n",
    "    #COEFICIENTES COEFK-----------------------\n",
    "    url_coefkest = f'https://api.esios.ree.es/archives/40/download_json?date_type=datos&end_date={fecha_fin}T23:59:59&start_date={fecha_ini}T00:00:00&locale=es'\n",
    "    cab = dict()\n",
    "    cab ['x-api-key'] = st.secrets['ESIOS_API_KEY']\n",
    "    response = requests.get(url_coefkest, headers=cab)\n",
    "    # Verificar el tipo de contenido de la respuesta\n",
    "    content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "    print(f\"🔍 Content-Type recibido: {content_type}\")\n",
    "\n",
    "    if \"zip\" in content_type or \"xls\" in content_type:\n",
    "        # Crear una carpeta donde extraer los archivos\n",
    "        extract_folder = \"coef_kest_data\"\n",
    "        os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "        # Leer el ZIP desde la respuesta\n",
    "        zip_buffer = io.BytesIO(response.content)\n",
    "\n",
    "        with zipfile.ZipFile(zip_buffer, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_folder)  # Extraer todos los archivos en la carpeta\n",
    "            extracted_files = zip_ref.namelist()  # Lista de archivos extraídos\n",
    "        \n",
    "        print(f\"✅ Archivo descargado y extraído en la carpeta '{extract_folder}'\")\n",
    "        print(f\"📂 Archivos extraídos: {extracted_files}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ Error: Se recibió otro tipo de archivo ({content_type})\")\n",
    "        print(response.text)\n",
    "    # Carpeta donde se extrajeron los archivos\n",
    "    extract_folder_coefk = \"coef_kest_data\"\n",
    "    # Listar los archivos `.xls` en la carpeta\n",
    "    xls_files = [f for f in os.listdir(extract_folder_coefk) if f.endswith(\".xls\")]\n",
    "    # Lista para almacenar los DataFrames\n",
    "    df_list = []\n",
    "    # Leer cada archivo y almacenarlo en la lista\n",
    "    for file in xls_files:\n",
    "        file_path = os.path.join(extract_folder_coefk, file)\n",
    "        # Leer el archivo Excel\n",
    "        df = pd.read_excel(file_path, skiprows=4)  # Cargar todas las hojas\n",
    "        # Eliminar las filas vacías al final\n",
    "        df = df.dropna(how='all')\n",
    "        # Eliminar las columnas: primera, tercera y cuarta (índices 0, 2, 3)\n",
    "        df = df.drop(df.columns[[0, 2, 3]], axis=1)\n",
    "        # Asegurar que no haya columnas extra manteniendo solo las primeras 25 (1 de fecha + 24 de horas)\n",
    "        if df.shape[1] > 25:\n",
    "            df = df.iloc[:, :25]\n",
    "        # Renombrar columnas\n",
    "        df.columns = ['fecha'] + list(range(24))\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenar todos los archivos en un solo DataFrame\n",
    "    df_coefk = pd.concat(df_list, ignore_index = True)\n",
    "    df_coefk['fecha'] = pd.to_datetime(df_coefk['fecha'], dayfirst=True)\n",
    "\n",
    "    # Opcional: Guardar en CSV\n",
    "    df_coefk.to_csv(\"COEF_KEST_MM_Combinado.csv\", index = False, sep = \";\")\n",
    "    print(\"📁 Archivo guardado: COEF_KEST_MM_Combinado.csv\")\n",
    "    df_datos = df_datos.copy()\n",
    "    df_coefk.columns = df_coefk.columns.astype(str)\n",
    "    df_datos = df_datos.merge(df_coefk, on='fecha', how='left')\n",
    "    # Extraer el coeficiente correspondiente a la columna de la hora\n",
    "    df_datos['coef_k'] = df_datos.apply(lambda row: row[str(row['hora'])], axis=1)\n",
    "    columnas_horas = [str(h) for h in range(24)]  # Generamos la lista de nombres de columnas de horas\n",
    "    df_datos.drop(columns = columnas_horas, inplace = True)\n",
    "\n",
    "    df_datos = df_datos.copy()\n",
    "    df_datos['perd_2.0'] = df_datos['perd_2.0_boe'] * df_datos['coef_k']\n",
    "    df_datos['perd_3.0'] = df_datos['perd_3.0_boe'] * df_datos['coef_k']\n",
    "    df_datos['perd_6.1'] = df_datos['perd_6.1_boe'] * df_datos['coef_k']\n",
    "    df_datos['coste_2.0'] = (df_datos['spot'] + df_datos['ssaa'] + df_datos['osom'] + df_datos['ppcc_2.0'] + df_datos['otros']) * 1.015 * (1 + df_datos['perd_2.0'])\n",
    "    df_datos['coste_3.0'] = (df_datos['spot'] + df_datos['ssaa'] + df_datos['osom'] + df_datos['ppcc_3.0'] + df_datos['otros']) * 1.015 * (1 + df_datos['perd_3.0'])\n",
    "    df_datos['coste_6.1'] = (df_datos['spot'] + df_datos['ssaa'] + df_datos['osom'] + df_datos['ppcc_6.1'] + df_datos['otros']) * 1.015 * (1 + df_datos['perd_6.1'])\n",
    "\n",
    "    df_datos['pyc_2.0'] = None\n",
    "    for _, row in df_pycs_energia.iterrows():\n",
    "        mask = (df_datos[\"fecha\"] >= row[\"fecha_inicio\"]) & (df_datos[\"fecha\"] <= row[\"fecha_final\"]) & (df_datos[\"dh_3p\"] == row[\"periodo\"])\n",
    "        df_datos.loc[mask, \"pyc_2.0\"] = row[\"2.0TD\"]\n",
    "    df_datos['pyc_3.0'] = None\n",
    "    for _, row in df_pycs_energia.iterrows():\n",
    "        mask = (df_datos[\"fecha\"] >= row[\"fecha_inicio\"]) & (df_datos[\"fecha\"] <= row[\"fecha_final\"]) & (df_datos[\"dh_6p\"] == row[\"periodo\"])\n",
    "        df_datos.loc[mask, \"pyc_3.0\"] = row[\"3.0TD\"]\n",
    "    df_datos['pyc_6.1'] = None    \n",
    "    for _, row in df_pycs_energia.iterrows():\n",
    "        mask = (df_datos[\"fecha\"] >= row[\"fecha_inicio\"]) & (df_datos[\"fecha\"] <= row[\"fecha_final\"]) & (df_datos[\"dh_6p\"] == row[\"periodo\"])\n",
    "        df_datos.loc[mask, \"pyc_6.1\"] = row[\"6.1TD\"]  \n",
    "    df_datos[['pyc_2.0', 'pyc_3.0', 'pyc_6.1']] *= 1000\n",
    "\n",
    "    df_datos['precio_2.0'] = df_datos['coste_2.0'] + df_datos['pyc_2.0']\n",
    "    df_datos['precio_3.0'] = df_datos['coste_3.0'] + df_datos['pyc_3.0']\n",
    "    df_datos['precio_6.1'] = df_datos['coste_6.1'] + df_datos['pyc_6.1']\n",
    "\n",
    "    return df_datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
